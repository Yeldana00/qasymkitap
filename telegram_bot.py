# telegram_bot.py
import pandas as pd
import psycopg2 # –ò—Å–ø–æ–ª—å–∑—É–µ–º psycopg2 –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import (
    Application, CommandHandler, MessageHandler, 
    filters, ContextTypes, CallbackQueryHandler
)
import logging
import os
from dotenv import load_dotenv

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ---
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)

# --- –ó–∞–≥—Ä—É–∑–∫–∞ —Å–µ–∫—Ä–µ—Ç–æ–≤ ---
load_dotenv() 
TOKEN = os.getenv("TOKEN")
DATABASE_URL = os.getenv("DATABASE_URL")

# --- (1) –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó POSTGRES –ò –ü–û–î–ì–û–¢–û–í–ö–ê –ú–û–î–ï–õ–ò ---

def load_data_from_db():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ PostgreSQL, —Å–æ–∑–¥–∞–µ—Ç TF-IDF –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏ –ú–∞—Ç—Ä–∏—Ü—É."""
    
    if not DATABASE_URL:
        logger.error("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: DATABASE_URL –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        exit()
        
    try:
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL...")
        with psycopg2.connect(DATABASE_URL) as conn:
            sql = "SELECT * FROM books"
            df = pd.read_sql_query(sql, conn)
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∫–Ω–∏–≥ –∏–∑ –ë–î.")
        
        # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
        df['–ê—Ç—ã'] = df['–ê—Ç—ã'].fillna('')
        df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] = df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].fillna('–ë–µ–ª–≥—ñ—Å—ñ–∑')
        df['–¢–æ–ª—ã“ì—ã—Ä–∞“õ'] = df['–¢–æ–ª—ã“ì—ã—Ä–∞“õ'].fillna('')
        df['–ê–≤—Ç–æ—Ä'] = df['–ê–≤—Ç–æ—Ä'].fillna('')

        category_mapping = {cat: i for i, cat in enumerate(df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].unique())}
        df['Category_Numeric'] = df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].map(category_mapping).fillna(-1)

        kazakh_stop_words = [
            '–∂”ô–Ω–µ', '–Ω–µ–º–µ—Å–µ', '–±—ñ—Ä–∞“õ', '“Ø—à—ñ–Ω', '–¥–µ–π—ñ–Ω', '–±–∞—Å—Ç–∞–ø', '–∂–æ“õ', '–±–∞—Ä', 
            '–±–∞—Ä–ª—ã“õ', '”ô—Ä', '–∫”©–ø', '–∞–∑', '–±–æ–ª–∞–¥—ã'
        ]
        
        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä (tfidf) ---
        tfidf_vectorizer = TfidfVectorizer(stop_words=kazakh_stop_words)
        
        df['combined'] = (
            df['–ê—Ç—ã'].str.lower() + ' ' + 
            df['–¢–æ–ª—ã“ì—ã—Ä–∞“õ'].str.lower() + ' ' + 
            df['–ê–≤—Ç–æ—Ä'].str.lower() + ' ' + 
            df['Category_Numeric'].astype(str)
        )
        
        # –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏ —Å–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –í–°–ï–• –∫–Ω–∏–≥
        tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined'])
        
        logger.info("–ú–æ–¥–µ–ª—å TF-IDF (–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏ –ú–∞—Ç—Ä–∏—Ü–∞) –≥–æ—Ç–æ–≤–∞!")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º DF, –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏ –ú–∞—Ç—Ä–∏—Ü—É
        return df, tfidf_vectorizer, tfidf_matrix

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î: {e}")
        exit()

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ 1 —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ ---
# –ò–ó–ú–ï–ù–ï–ù–ò–ï: —Å–æ—Ö—Ä–∞–Ω—è–µ–º tfidf (–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä) –∏ tfidf_matrix
books_df, tfidf, tfidf_matrix = load_data_from_db()

# --- (2) –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê –ü–û–ò–°–ö–ê –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú ---

def find_books_by_keywords(query: str) -> pd.DataFrame:
    """
    –ò—â–µ—Ç –∫–Ω–∏–≥–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–∑ –∑–∞–ø—Ä–æ—Å–∞, —Å—Ä–∞–≤–Ω–∏–≤–∞—è
    –∑–∞–ø—Ä–æ—Å —Å–æ –≤—Å–µ–º–∏ –∫–Ω–∏–≥–∞–º–∏ –≤ tfidf_matrix.
    """
    try:
        query = query.lower()
        
        # 1. –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        #    –ò—Å–ø–æ–ª—å–∑—É–µ–º .transform() (–Ω–µ .fit_transform()), 
        #    —Ç–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞
        query_vector = tfidf.transform([query])
        
        # 2. –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å (cosine similarity) –∑–∞–ø—Ä–æ—Å–∞ —Å–æ –í–°–ï–ú–ò –∫–Ω–∏–≥–∞–º–∏
        #    (query_vector: 1xN, tfidf_matrix: 177xN) -> (scores: 1x177)
        scores = cosine_similarity(query_vector, tfidf_matrix).flatten()
        
        # 3. –ù–∞—Ö–æ–¥–∏–º 5 –ª—É—á—à–∏—Ö (non-zero) —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if scores.max() == 0:
            return pd.DataFrame() # –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ

        # argsort() —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É
        # [:-6:-1] –±–µ—Ä–µ—Ç 5 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö (—Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö) –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        top_indices = scores.argsort()[:-6:-1]
        
        # 4. –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö 0 —Å—Ö–æ–∂–µ—Å—Ç—å
        top_scores = scores[top_indices]
        valid_indices = top_indices[top_scores > 0]
        
        if len(valid_indices) == 0:
            return pd.DataFrame()
            
        return books_df.iloc[valid_indices]
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ find_books_by_keywords: {e}")
        return pd.DataFrame()


def format_book_message(book: pd.Series) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—ã–≤–æ–¥ –∫–Ω–∏–≥–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    if pd.notna(book['–ë–∞“ì–∞—Å—ã']):
        price = f"{book['–ë–∞“ì–∞—Å—ã']:,.0f} —Ç–≥".replace(',', ' ')
    else:
        price = "–ë–∞“ì–∞—Å—ã –±–µ–ª–≥—ñ—Å—ñ–∑"
    
    message = (
        f"üìö <b>{book['–ê—Ç—ã']}</b>\n"
        f"üë§ –ê–≤—Ç–æ—Ä—ã: {book['–ê–≤—Ç–æ—Ä']}\n"
        f"üóÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {book['–ö–∞—Ç–µ–≥–æ—Ä–∏—è']}\n"
        f"üí∞ –ë–∞“ì–∞—Å—ã: {price}\n"
    )
    if pd.notna(book['URL']):
         message += f"üîó <a href=\"{book['URL']}\">–°–∞–π—Ç—Ç–∞–Ω “õ–∞—Ä–∞—É</a>\n"
    return message + "\n"

# --- (3) –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò TELEGRAM ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ /start (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    user = update.effective_user
    categories = [cat for cat in books_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].unique() if cat != '–ë–µ–ª–≥—ñ—Å—ñ–∑']
    top_categories = categories[:9] 
    
    keyboard = []
    row = []
    for category in top_categories:
        row.append(InlineKeyboardButton(category, callback_data=f"cat_{category}"))
        if len(row) == 3:
            keyboard.append(row)
            row = []
    if row: keyboard.append(row)
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_html(
        f"–°”ô–ª–µ–º, {user.mention_html()}! üëã\n\n"
        f"–¢”©–º–µ–Ω–¥–µ–≥—ñ —Å–∞–Ω–∞—Ç—Ç—ã (–∫–∞—Ç–µ–≥–æ—Ä–∏—è–Ω—ã) —Ç–∞“£–¥–∞“£—ã–∑, –Ω–µ–º–µ—Å–µ –∫—ñ—Ç–∞–ø –∞—Ç–∞—É—ã–Ω/—Å–∏–ø–∞—Ç—Ç–∞–º–∞—Å—ã–Ω –∂–∞–∑—ã“£—ã–∑:", # "–∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ/–æ–ø–∏—Å–∞–Ω–∏–µ –∫–Ω–∏–≥–∏"
        reply_markup=reply_markup
    )

async def handle_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, 
    –ò–°–ü–û–õ–¨–ó–£–Ø –ù–û–í–£–Æ –§–£–ù–ö–¶–ò–Æ –ü–û–ò–°–ö–ê
    """
    query_text = update.message.text
    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏—â–µ—Ç –ø–æ –∫–ª—é—á–∞–º: '{query_text}'")
    try:
        # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í—ã–∑—ã–≤–∞–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é
        found_books = find_books_by_keywords(query_text)
        
        if found_books.empty:
            await update.message.reply_text("–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, –æ—Å—ã —Å”©–∑–¥–µ—Ä –±–æ–π—ã–Ω—à–∞ –µ—à—Ç–µ“£–µ —Ç–∞–±—ã–ª–º–∞–¥—ã. üòï") # "Sorry, nothing found for these words"
            return
            
        # –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ú–µ–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç–≤–µ—Ç–∞
        response_message = f"<b>'{query_text}'</b> —Å”©–∑–¥–µ—Ä—ñ –±–æ–π—ã–Ω—à–∞ —ñ–∑–¥–µ—É –Ω”ô—Ç–∏–∂–µ–ª–µ—Ä—ñ:\n\n" # "Search results for the words:"
        for _, book in found_books.iterrows():
            response_message += format_book_message(book)
        await update.message.reply_html(response_message, disable_web_page_preview=True)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –∫–ª—é—á–∞–º: {e}")
        await update.message.reply_text("–û–π, –±—ñ—Ä “õ–∞—Ç–µ–ª—ñ–∫ –æ—Ä—ã–Ω –∞–ª–¥—ã.")

async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–Ω–æ–ø–æ–∫ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    query = update.callback_query
    await query.answer()
    callback_data = query.data
    
    if callback_data.startswith("cat_"):
        category_name = callback_data.split("_", 1)[1]
        logger.info(f"–ù–∞–∂–∞—Ç–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category_name}")
        category_books = books_df[books_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] == category_name]
        
        if category_books.empty:
            await query.message.reply_text(f"<b>{category_name}</b> —Å–∞–Ω–∞—Ç—ã–Ω–¥–∞ –∫—ñ—Ç–∞–ø—Ç–∞—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã.", parse_mode='HTML')
            return
            
        sample_books = category_books.sample(min(5, len(category_books)))
        response_message = f"<b>{category_name}</b> —Å–∞–Ω–∞—Ç—ã–Ω–¥–∞“ì—ã –∫–µ–∑–¥–µ–π—Å–æ“õ –∫—ñ—Ç–∞–ø—Ç–∞—Ä:\n\n"
        for _, book in sample_books.iterrows():
            response_message += format_book_message(book)
        await query.message.reply_html(response_message, disable_web_page_preview=True)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
    if not TOKEN:
        logger.error("!!! –¢–û–ö–ï–ù –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù !!!")
        logger.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ .env —Ñ–∞–π–ª –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Ö–æ—Å—Ç–∏–Ω–≥–∞.")
        return

    application = Application.builder().token(TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CallbackQueryHandler(button_callback))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text_message))

    logger.info("–ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    application.run_polling()

if __name__ == "__main__":
    main()
